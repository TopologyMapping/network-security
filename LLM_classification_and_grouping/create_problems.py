"""
This script processes the classification answers generated by the LLM model.

Prerequisites:
- The `distributed_classification.py` script must be run beforehand to generate the classification answers.
- Multiple classification answers are required to ensure accurate grouping of the scripts.

Functionality:
- Groups scripts into categories referred to as 'problems' based on the classification answers.
- Identifies and records scripts that could not be grouped under 'errors'.

Output:
- A JSON file containing:
  - `problems`: The categorized groups of scripts.
  - `errors`: Scripts that could not be categorized.
"""

import argparse
import dataclasses
import json
import os
import re
from collections import defaultdict

import nltk
from dataclasses_json import dataclass_json
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from aux.nuclei import extract_nuclei_id, parse_nuclei_yaml
from aux.openvas import extract_oid_openvas
from aux.utils import read_file_with_fallback

nltk.download("punkt_tab")
nltk.download("stopwords")
STOP_WORDS = set(stopwords.words("english"))

RESULTS_DIRECTORY_NAME = "./results"

CLASSIFICATION_FOLDER = "./classification"

VALUES_WHAT_IS_DETECTED = [
    "Vulnerability",
    "Old Software",
    "Properties of a System",
    "Unmaintained Software",  # these values are not the correct categories, but they are present on the results
    "Property of a System",
]

VALUES_CATEGORY = ["SimulatedAttack", "PrivilegedScan", "BasicActiveRequests"]

VALUES_SUBCATEGORY = [
    "ExternalCodeExecution",
    "UnauthorizedLogin",
    "ProtectedInformation",
    "DenialofServiceDoS",
    "PrivilegedAttack",
    "PackageList",
    "Serviceinformation",
    "LogFileAnalysis",
    "BannerCheck",
    "URLPresenceCheck",
    "Discovery",
]


# Classes to handle information storage
@dataclass_json
@dataclasses.dataclass
class FileInfo:
    classification_file_name: str
    vulnerability_tool_script_name: str
    script_id: (
        str  # the id is used to store the information in the Defect Dojo application
    )


@dataclass_json
@dataclasses.dataclass(frozen=True)
class CategorySubcategory:
    category: str
    subcategory: str


@dataclass_json
@dataclasses.dataclass(frozen=True)
class KeysProblemsInfo:
    cve: str
    attack: CategorySubcategory
    vuln_type: str
    vuln_name: tuple


@dataclasses.dataclass
class ClassificationResult:
    cves: list
    file: str
    id: str
    classification: str


@dataclasses.dataclass
class ClassificationInfo:
    what_is_detected: str = ""
    application: str = ""
    category: str = ""
    subcategory: str = ""


# Classes to handle exceptions
class RegexError(Exception):
    """Exception raised for errors related to regular expressions."""

    pass


class LLMError(Exception):
    """Exception raised for errors related to LLM processing."""

    pass


CLASSIFICATION_RESULTS_FOLDER = "./classification"

LLM_ERROR = "Error in LLM answer"
REGEX_ERROR = "Error in regex match"

TASK1_PATTERN = (
    r"What is detected:\s*\n*\{?(.*?)\}?\s*\n*A:\s*(.*?)\s*\n*B:\s*\{?(.*?)\}?"
)
TASK2_PATTERN = r"Category:\s*\{?(.*?)\}?\s*Subcategory:\s*\{?(.*?)\}?\s*(?:\n|$)"
NON_ALPHANUMERIC_REGEX = r"[^\w\s]"
NUMERIC_REGEX = r"\d+"
CATEGORY_SUBCATEGORY_CLEANUP_REGEX = r"[ .\n]"


def extract_task_information(classification_text) -> dict:
    """
    Extracts Task 1 and Task 2 information from the classification text.
    Task 1 refers to the 'what is detected', 'application' and 'version' fields.
    Task 2 refers to the 'category' and 'subcategory' fields.

    The 'version' could be used to improve the grouping, but it is not used in this implementation, because the values are too variables, so its hard to match the scripts based on this field. But in the future it can be improved.

    Input: the LLM classification answer.
    Output: a dictionary with the extracted information.
    """

    # Search for Task 1 and Task 2 using regex patterns
    task1_match = re.search(TASK1_PATTERN, classification_text, re.DOTALL)
    task2_match = re.search(TASK2_PATTERN, classification_text, re.DOTALL)

    # Process Task 1 fields
    task1_what_is_detected = (
        re.sub(NON_ALPHANUMERIC_REGEX, "", task1_match.group(1).strip())
        if task1_match
        else " "
    )
    task1_application = (
        re.sub(NON_ALPHANUMERIC_REGEX, "", task1_match.group(2).strip())
        if task1_match
        else " "
    )

    # Process Task 2 fields
    task2_category = (
        re.sub(NON_ALPHANUMERIC_REGEX, "", task2_match.group(1).strip())
        if task2_match
        else " "
    )
    task2_subcategory = (
        re.sub(NON_ALPHANUMERIC_REGEX, "", task2_match.group(2).strip())
        if task2_match
        else " "
    )

    # Clean up values
    task2_category = task2_category.replace("Category", "")
    task2_subcategory = task2_subcategory.replace("Subcategory", "")

    task1_what_is_detected = re.sub(NUMERIC_REGEX, "", task1_what_is_detected).replace(
        "\n", ""
    )
    task1_application = re.sub(NUMERIC_REGEX, "", task1_application).replace("\n", "")

    if task1_what_is_detected == "Property of a System":
        task1_what_is_detected = "Properties of a System"

    task2_category = (
        re.sub(NUMERIC_REGEX, "", task2_category)
        .replace(".", "")
        .replace(" ", "")
        .replace("\n", "")
    )
    task2_subcategory = (
        re.sub(NUMERIC_REGEX, "", task2_subcategory)
        .replace(".", "")
        .replace(" ", "")
        .replace("\n", "")
    )

    # Validation checks
    if task2_subcategory == "" or task2_category == "" or task1_what_is_detected == "":
        raise RegexError()

    if (
        task2_subcategory not in VALUES_SUBCATEGORY
        or task2_category not in VALUES_CATEGORY
        or task1_what_is_detected not in VALUES_WHAT_IS_DETECTED
    ):
        raise LLMError()

    # Return extracted information
    information = {
        "what_is_detected": task1_what_is_detected,
        "application": task1_application,
        "category": task2_category,
        "subcategory": task2_subcategory,
    }

    return information


def check_if_scripts_application_contains_similar_tokens(
    application_tokens_list: list, filtered_tokens: list
) -> str:
    for token in application_tokens_list:
        sub_tokens = token.split("_")
        for sub_token in sub_tokens:
            for filtered_token in filtered_tokens:
                if sub_token in filtered_token or filtered_token in sub_token:
                    return token
    return ""


def sort_problems(problems: dict):
    """
    This function sorts the problems dictionary by the length of the innermost list. There is no need to do this, but it is useful to see the results of the grouping.
    """
    sorted_problems = {}
    for cve, cve_dicts in problems.items():
        sorted_problems[cve] = {}
        for task2, task2_dict in cve_dicts.items():
            sorted_problems[cve][task2] = {}
            for (
                task1_what_is_detected,
                task1_what_is_detected_dict,
            ) in task2_dict.items():
                # Sort by the length of the innermost list
                sorted_problems[cve][task2][task1_what_is_detected] = dict(
                    sorted(task1_what_is_detected_dict.items(), key=lambda x: len(x[1]))
                )
    return sorted_problems


def filter_classification_text(
    classification_text, errors_llm: list, errors_regex: list, file_info: FileInfo
) -> ClassificationInfo:
    file_info_dict = file_info.to_dict()

    try:
        info = extract_task_information(classification_text)
        return ClassificationInfo(
            what_is_detected=info.get("what_is_detected", ""),
            application=info.get("application", ""),
            category=info.get("category", ""),
            subcategory=info.get("subcategory", ""),
        )
    except RegexError:
        errors_regex.append(file_info_dict)
    except LLMError:
        errors_llm.append(file_info_dict)
    except Exception as e:
        print("Error: ", e)

    return ClassificationInfo()


def grouping_info(
    problems: dict,
    cves: list,
    classification_category_subcategory: CategorySubcategory,
    classification_what_is_detected: str,
    application: str,
    file_info: FileInfo,
    errors_llm: list,
):
    """
    This function groups the information extracted from the classification text.
    The ideia is to group the classified scripts by the clasification performed by the LLM model:
        - CVE (deterministic information, the most constant)
            - Task2 (category and subcategory) -> not so constant, but the LLM is restricted to a few options
                - Task1 (what is detected) -> the same case as before
                    - Task1 (application name) -> the most variable information. Could be any string

    To know more about the classification, Taks1 and Taks2, see the file 'distributed_classification.py' and the folder 'prompts'.

    To handle the variability of the application name, the value is tokenized and then is compared the similarity between the tokens of the application name and the tokens of the already classified scripts. If the current script contains similar tokens, then they are grouped together. Otherwise, a new group is created.

    This functions performs changes in the 'problems' dictionary and in the 'errors_llm' list. So nothing is returned.

    """

    file_info = (
        file_info.to_dict()
    )  # this will help to store information in a JSON file

    # starting grouping by CVE -> most constant info
    for cve in cves:

        if cve not in problems:
            problems[cve] = {}

        # grouping by constant info again (task2 refers to the category and subcategory)
        if classification_category_subcategory not in problems[cve]:
            problems[cve][classification_category_subcategory] = {}

        if (
            classification_what_is_detected
            not in problems[cve][classification_category_subcategory]
        ):
            problems[cve][classification_category_subcategory][
                classification_what_is_detected
            ] = {}

        # problems = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))

        tokens_application = word_tokenize(application.lower())

        filtered_tokens_application = [
            word for word in tokens_application if word.lower() not in STOP_WORDS
        ]

        # if the application name contains too much tokens, it is not useful for grouping (could be a LLM error). The value 6 is arbitrary
        check_if_application_name_is_too_long = len(filtered_tokens_application)
        if check_if_application_name_is_too_long > 6:
            errors_llm.append(file_info)
            return

        # starting grouping by the classified application name. Too variable info, so it is the last to be grouped

        # storing the tokens in a string to be used as a key
        # (the key is a string because the information is too variable, so it is not possible to use a dataclass as a key)
        key_tokens_taks1_application = "_".join(filtered_tokens_application) + "_"

        application_tokens_list = problems[cve][classification_category_subcategory][
            classification_what_is_detected
        ].keys()

        match_application_tokens = check_if_scripts_application_contains_similar_tokens(
            application_tokens_list, filtered_tokens_application
        )

        if match_application_tokens:
            problems[cve][classification_category_subcategory][
                classification_what_is_detected
            ][match_application_tokens].append(file_info)
            return

        problems[cve][classification_category_subcategory][
            classification_what_is_detected
        ][key_tokens_taks1_application] = []
        problems[cve][classification_category_subcategory][
            classification_what_is_detected
        ][key_tokens_taks1_application].append(file_info)

    return


def organizing_grouping_structure(result: dict):
    """
    This function is responsible for organizing the grouping structure of the scripts. The structure presented before as CVES -> Task2 -> Task1 -> Application on the function 'process_json_files' is good to improve the understanding of the grouping. But to store the information to be used in Defect Dojo application, it is better to store the information as a list of scripts grouped together.

    The scripts names are changed to the respective ID, so the information could be used in the Defect Dojo application.

    The 'metasploit' files are ignored because they are not used in the Defect Dojo application. But if this could be possible in the future, the 'metasploit' files could be included in the grouping structure.

    The output is stored in ./results/problems.json
    """
    organized_grouping = defaultdict(list)

    # Traverse the nested dictionary structure -> there is not too much information to be extracted (in practice, the code is not O(n^5)). The CVE could contain multiple values, but the other keys are more limited
    for cve, attacks in result["problems"].items():
        for attack, attack_details in attacks.items():
            for vuln_type, vuln_paths in attack_details.items():
                for vuln_name, files_info in vuln_paths.items():
                    for file_info in files_info:

                        # Create the structured key using the dataclass
                        value = KeysProblemsInfo(
                            cve=cve,
                            attack=attack,
                            vuln_type=vuln_type,
                            vuln_name=(
                                tuple(vuln_name.split())
                                if isinstance(vuln_name, str)
                                else vuln_name
                            ),
                        )

                        script_name = file_info["vulnerability_tool_script_name"]

                        if "metasploit" in script_name:
                            continue

                        id = file_info["script_id"]

                        organized_grouping[value].append(id)

    # removing the keys that have only one value, because they are not grouped
    keys_to_delete = [
        key
        for key, value in organized_grouping.items()
        if isinstance(value, (list, set)) and len(value) == 1
    ]
    for key in keys_to_delete:
        del organized_grouping[key]

    # transforming dataclasses in dicts
    organized_grouping_json = {
        str(key): value for key, value in organized_grouping.items()
    }

    directory_path = os.path.abspath(RESULTS_DIRECTORY_NAME)
    os.makedirs(directory_path, exist_ok=True)
    file_path = os.path.join(directory_path, "problems.json")

    print("Saving results in ", file_path)
    with open(file_path, "w") as f:
        json.dump(organized_grouping_json, f, indent=4)


def process_json_files(folder_path: str):
    """
    Processes JSON files in the folder and extracts required information.
    :param folder_path: Path to the folder containing JSON files.
    :param output_folder: Path to the folder to save the processed files.

    Important: The input file must be a JSON file containing the classification information of the scripts, performed by the code in the 'distributed_classification.py' script.

    The output is stored in ./results/grouped_scripts.json

    """

    problems: dict = {}
    errors_llm: list = []
    errors_regex: list = []

    for file_name in os.listdir(folder_path):
        if file_name.endswith(".json"):
            file_path = os.path.join(folder_path, file_name)

            with open(file_path, "r") as file:
                data = json.load(file)

            for scan_app in data.keys():
                if scan_app == "tests_with_no_CVE":
                    continue

                for key, list_classification in data[
                    scan_app
                ].items():
                    if key == "scripts_without_cves":
                        continue
                    
                    classification_results = [
                        ClassificationResult(
                            cves=item.get("cves"),
                            file=item.get("file"),
                            id=item.get("id"),
                            classification=item.get("classification"),
                        )
                        for item in list_classification
                    ]

                    for result in classification_results:
                        cves = result.cves

                        # if there is no CVE, it is stored as an empty string to be used as a key in the dictionary
                        if cves is None or cves == []:
                            cves = ["No-CVE"]
                        elif isinstance(cves, str):
                            cves = [cves]

                        # storing results of grouping as the classificaiton file where the script was classified together with the script name
                        file_info = FileInfo(
                            classification_file_name=file_name,
                            vulnerability_tool_script_name=result.file,
                            script_id=result.id,
                        )

                        classification_info_extracted = filter_classification_text(
                            result.classification,
                            errors_llm,
                            errors_regex,
                            file_info,
                        )

                        # Skip if extraction failed (empty ClassificationInfo)
                        ci = classification_info_extracted
                        if not ci or (isinstance(ci, ClassificationInfo) and not any(dataclasses.asdict(ci).values())):
                            continue

                        wd = ci.what_is_detected
                        app = ci.application
                        cat = ci.category
                        subcat = ci.subcategory

                        cat_subcat = CategorySubcategory(category=cat, subcategory=subcat)

                        grouping_info(
                            problems,
                            cves,
                            cat_subcat,
                            wd,
                            app,
                            file_info,
                            errors_llm,
                        )

    # transform dataclasses in dicts
    serializable_problems = {
        cve: {
            str(category_subcategory): inner_dict
            for category_subcategory, inner_dict in classification.items()
        }
        for cve, classification in problems.items()
    }

    sorted_problems = sort_problems(serializable_problems)

    result = {
        "len_problems": len(sorted_problems),
        "len_errors_llm": len(errors_llm),
        "len_errors_regex": len(errors_regex),
        "problems": sorted_problems,
        "errors_llm": errors_llm,
        "errors_regex": errors_regex,
    }

    directory_path = os.path.abspath(RESULTS_DIRECTORY_NAME)
    os.makedirs(directory_path, exist_ok=True)
    file_path = os.path.join(directory_path, "grouped_scripts.json")

    print("Saving results in ", file_path)
    with open(file_path, "w") as f:
        json.dump(result, f, indent=4)

    organizing_grouping_structure(result)


if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description="Match classification between Nmap, OpenVAS, and Nuclei"
    )
    parser.add_argument(
        "--input",
        required=False,
        default=CLASSIFICATION_FOLDER,
        help="input folder with classification results from 'distributed_classification.py'. The default folder is [%(default)s], but inform this argument if the files are stored in other place.",
    )

    args = parser.parse_args()

    # Process JSON files
    process_json_files(args.input)
